{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MlwXvK1fcq57",
    "outputId": "98c96dfe-ba06-4c84-b93b-7acde4efef1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "B7h1X2wXcq5_"
   },
   "outputs": [],
   "source": [
    "#구현하는 모델에서 쓰이는 모든 activation함수는 정의하여 드린 GELU 함수를 사용해야함.\n",
    "#MultiHeadAttention에서 Head로 나눌때, 이미지를 patch로자른후 sequence로 만들때 Rearrange함수를 사용하면 편리함.(사용하지 않으셔도 됩니다)\n",
    "#CIFAR10에 대한 test accuracy가 60프로 이상인 ViT모델을 만드시오.\n",
    "import tensorflow as tf\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "from tensorflow.keras.activations import gelu\n",
    "GELU = lambda x : gelu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IIDyfcUVcq6A"
   },
   "outputs": [],
   "source": [
    "#논문[1]에서 설명하는 MultiHeadAttention을 만들어라.\n",
    "class MultiHeadedAttention(tf.keras.Model):\n",
    "    #dimension - 모델의 dimension(MHA를 거친 후의 dimension)\n",
    "    def __init__(self, dimension, heads=8):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        ############Write your code Here############ \n",
    "        \n",
    "        self.heads=heads # head의 개수 \n",
    "        self.scales=dimension**-0.5 #분모(scale 역할)\n",
    "        \n",
    "        self.vkq=tf.keras.layers.Dense(dimension*3,use_bias=False) ## value key query\n",
    "        self.out=tf.keras.layers.Dense(dimension)\n",
    "        \n",
    "        self.rearrange_vkq=Rearrange('b n (vkq h d) -> vkq b h n d',vkq=3,h=self.heads)\n",
    "        \n",
    "        self.rearrange_out=Rearrange('b h n d -> b n (h d)')\n",
    "        \n",
    "        ############################################\n",
    "    def call(self, inputs):\n",
    "        output = None\n",
    "        ############Write your code Here############\n",
    "        \n",
    "        vkq=self.vkq(inputs)\n",
    "        vkq=self.rearrange_vkq(vkq)\n",
    "        \n",
    "        v=vkq[2];k=vkq[1];q=vkq[0]\n",
    "        \n",
    "        ## Attention(Q,K,V)=softmax(QKt/√dk)*V       \n",
    "\n",
    "        qk_matrix=tf.einsum('abid,abjd->abij',q,k)*self.scales \n",
    "        softmax=tf.nn.softmax(qk_matrix,axis=-1)        \n",
    "        output=tf.einsum('abij,abjd->abid',softmax,v)\n",
    "        \n",
    "        \n",
    "        output=self.rearrange_out(output)\n",
    "        output=self.out(output)\n",
    " \n",
    "        ############################################\n",
    "        return output\n",
    "\n",
    "#인자로 받은 residual_function을 사용하여 real_function값을 return하여주는 Class를 만들어라.(call함수 참고)\n",
    "class ResidualBlock(tf.keras.Model):\n",
    "    def __init__(self, residual_function):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        ############Write your code Here############\n",
    "        \n",
    "        self.residual_function=residual_function\n",
    "        \n",
    "        ############################################\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.residual_function(inputs) + inputs\n",
    "\n",
    "#인자로 받은 normfunction에 들어가기전에 LayerNormalization을 해주는 Class를 만들어라.(call함수 참고)\n",
    "class NormalizationBlock(tf.keras.Model):\n",
    "    def __init__(self, norm_function, epsilon=1e-5):\n",
    "        super(NormalizationBlock, self).__init__()\n",
    "        ############Write your code Here############\n",
    "         \n",
    "        self.normalize=tf.keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.norm_function=norm_function\n",
    "        \n",
    "        ############################################\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.norm_function(self.normalize(inputs))\n",
    "\n",
    "#논문[1]에서의 MLPBlock을 만들어라.\n",
    "class MLPBlock(tf.keras.Model):\n",
    "    #output_dimension - MLPBlock의 output dimension\n",
    "    #hidden_dimension - MLPBlock의 hidden layer dimension\n",
    "    def __init__(self, output_dimension, hidden_dimension):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        ############Write your code Here############\n",
    "        \n",
    "        self.linear=tf.keras.Sequential([tf.keras.layers.Dense(hidden_dimension,activation=gelu),\n",
    "                                     tf.keras.layers.Dense(output_dimension)])\n",
    "        \n",
    "        ############################################\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = None\n",
    "        ############Write your code Here############\n",
    "        \n",
    "        output=self.linear(inputs)\n",
    "        \n",
    "        ############################################\n",
    "        return output\n",
    "\n",
    "#논문[1]을 읽고 TransformerEncoder를 위에서 정의한 class들을 사용하여 만들어라.\n",
    "class TransformerEncoder(tf.keras.Model):\n",
    "    #dimension - 모델의 dimension(MHA를 거친 후의 dimension), heads - MHA에서 head의 개수\n",
    "    #depth - encoder layer의 개수, mlp_dimension - MLP block의 hidden layer의 dimension\n",
    "    def __init__(self, dimension, depth, heads, mlp_dimension): \n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        layers_ = []\n",
    "        for _ in range(depth):\n",
    "            ############Write your code Here############ \n",
    "            \n",
    "            layers_=layers_+[ResidualBlock(NormalizationBlock(MultiHeadedAttention(dimension,heads=heads)))]       \n",
    "            layers_=layers_+[ResidualBlock(NormalizationBlock(MLPBlock(dimension,mlp_dimension)))]\n",
    "            \n",
    "            ############################################\n",
    "        self.layers_ = tf.keras.Sequential(layers_)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.layers_(inputs)\n",
    "\n",
    "#논문[2]를 읽고 ViT모델을 위에서 정의한 class들을 사용하여 만들어라.\n",
    "class ImageTransformer(tf.keras.Model):\n",
    "    #image_size - 이미지의 W==H의 크기(int), patch_size - 이미지를 쪼갤 patch의 크기(int)\n",
    "    #n_classes - 최종 class의 개수, batch_size - 배치사이즈\n",
    "    #dimension - 모델의 dimension(MHA를 거친 후의 dimension), depth - encoder layer의 개수\n",
    "    #heads - MHA에서 head의 개수, mlp_dimension - MLP block의 hidden layer의 dimension\n",
    "    #channel - input image에 대한 channel의 수\n",
    "    def __init__(\n",
    "            self, image_size, patch_size, n_classes, batch_size,\n",
    "            dimension, depth, heads, mlp_dimension, channels=3):\n",
    "        super(ImageTransformer, self).__init__()\n",
    "        assert image_size % patch_size == 0, 'invalid patch size for image size'\n",
    "\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.dimension = dimension\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.positional_embedding = self.add_weight(\n",
    "            \"position_embeddings\", shape=[num_patches + 1, dimension],\n",
    "            initializer=tf.keras.initializers.RandomNormal(), dtype=tf.float32\n",
    "        )\n",
    "        self.classification_token = self.add_weight(\n",
    "            \"classification_token\", shape=[1, 1, dimension],\n",
    "            initializer=tf.keras.initializers.RandomNormal(), dtype=tf.float32\n",
    "        )\n",
    "        ############Write your code Here############\n",
    "        \n",
    "        self.embedding=tf.keras.layers.Dense(dimension)\n",
    "        \n",
    "        self.rearrange=Rearrange('b c (h p_1) (w p_2) -> b (h w) (p_1 p_2 c)',p_1=self.patch_size,p_2=self.patch_size)\n",
    "        \n",
    "        self.transformer=TransformerEncoder(dimension,depth,heads,mlp_dimension)\n",
    "        \n",
    "        self.cls=tf.identity\n",
    "        \n",
    "        self.mlp=tf.keras.Sequential([tf.keras.layers.Dense(mlp_dimension,activation=gelu),tf.keras.layers.Dense(n_classes)])\n",
    "        \n",
    "        \n",
    "        ############################################\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = None\n",
    "        ############Write your code Here############\n",
    "        \n",
    "        output=self.rearrange(inputs)\n",
    "        output=self.embedding(output)\n",
    "\n",
    "        cls=tf.broadcast_to(self.classification_token,(tf.shape(inputs)[0],1,self.dimension))\n",
    "        \n",
    "        output=tf.concat((cls,output),axis=1)\n",
    "        output=output+self.positional_embedding\n",
    "        output=self.transformer(output)\n",
    "        \n",
    "        output=self.cls(output[:,0])\n",
    "        output=self.mlp(output)\n",
    "        \n",
    "        ############################################\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hzOcwCYcq6C",
    "outputId": "6411d46f-4cfb-4004-c4c8-2f424de5f1a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 6s 0us/step\n",
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 54s 29ms/step - loss: 1.6753 - accuracy: 0.3772\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 1.3703 - accuracy: 0.5003\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 1.2744 - accuracy: 0.5396\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 1.2126 - accuracy: 0.5603\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 1.1615 - accuracy: 0.5797\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 1.1160 - accuracy: 0.5953\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 1.0775 - accuracy: 0.6090\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 1.0435 - accuracy: 0.6253\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 1.0096 - accuracy: 0.6340\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.9774 - accuracy: 0.6477\n",
      "==============Training Finished===============\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 1.1059 - accuracy: 0.6001\n",
      "Test Accuracy : 0.6000999808311462\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "# Download and prepare the CIFAR10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "############Write your code Here############\n",
    "\n",
    "train_images,test_images=train_images/255.0,test_images/255.0\n",
    "\n",
    "############################################\n",
    "# Make image shape (BS, H, W, C) to (BS, C, H, W)\n",
    "############Write your code Here############\n",
    "\n",
    "train_images=tf.transpose(train_images,perm=[0,3,1,2])\n",
    "test_images=tf.transpose(test_images,perm=[0,3,1,2])\n",
    "\n",
    "############################################\n",
    "\n",
    "#Initialize your model\n",
    "#Initialize optimizer and loss and compile it to the model\n",
    "############Write your code Here############\n",
    "\n",
    "\n",
    "config={\"image_size\":32,\"patch_size\":4,\"n_classes\":10,\"batch_size\":1,\n",
    "        \"dimension\":64,\"depth\":3,\"heads\":4,\"mlp_dimension\":128,\"channels\":3}\n",
    "    \n",
    "model=ImageTransformer(**config)\n",
    "model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "        \n",
    "############################################\n",
    "\n",
    "#Train your model\n",
    "############Write your code Here############\n",
    "\n",
    "model.fit(train_images,train_labels,epochs=10)\n",
    "\n",
    "############################################\n",
    "print('==============Training Finished===============')\n",
    "\n",
    "#Evaluate your test samples\n",
    "accuracy = 0\n",
    "############Write your code Here############\n",
    "\n",
    "_,accuracy=model.evaluate(test_images,test_labels)\n",
    "\n",
    "############################################\n",
    "\n",
    "print('Test Accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVXQkmbicq6C"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DM_Assignment_5_120210211 이지환.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
